{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2517be49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in JSON: 340\n",
      "Images with ≥1 valid segmentation: 340\n",
      "Copied: 340\n",
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import json, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def has_seg(a):\n",
    "    s = a.get(\"segmentation\")\n",
    "    if isinstance(s, list):   # polygons\n",
    "        return len(s) > 0\n",
    "    if isinstance(s, dict):   # RLE\n",
    "        return bool(s.get(\"counts\")) and bool(s.get(\"size\"))\n",
    "    return False\n",
    "\n",
    "def copy_images_with_seg(\n",
    "    annotation_json: str,\n",
    "    source_dir: str,\n",
    "    dest_dir: str,\n",
    "    exts=(\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "):\n",
    "    ann = json.loads(Path(annotation_json).read_text(encoding=\"utf-8\"))\n",
    "    imgs = ann.get(\"images\", [])\n",
    "    anns = ann.get(\"annotations\", [])\n",
    "\n",
    "    # ids of images that have at least one valid segmentation\n",
    "    ids_with_seg = {a[\"image_id\"] for a in anns if \"image_id\" in a and has_seg(a)}\n",
    "\n",
    "    # map image id -> file_name (basename)\n",
    "    id2name = {img[\"id\"]: Path(str(img[\"file_name\"]).strip()).name for img in imgs if \"id\" in img and \"file_name\" in img}\n",
    "\n",
    "    # target basenames, lowercased\n",
    "    targets = {id2name[i].lower() for i in ids_with_seg if i in id2name}\n",
    "\n",
    "    src_root = Path(source_dir)\n",
    "    dst_root = Path(dest_dir)\n",
    "    dst_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # index all source images by basename (case-insensitive)\n",
    "    index = {}\n",
    "    for p in src_root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in exts:\n",
    "            index[p.name.lower()] = p\n",
    "\n",
    "    copied, missing = 0, []\n",
    "    for name in sorted(targets):\n",
    "        src = index.get(name)\n",
    "        if src:\n",
    "            shutil.copy2(src, dst_root / src.name)\n",
    "            copied += 1\n",
    "        else:\n",
    "            missing.append(name)\n",
    "\n",
    "    print(f\"Images in JSON: {len(imgs)}\")\n",
    "    print(f\"Images with ≥1 valid segmentation: {len(ids_with_seg)}\")\n",
    "    print(f\"Copied: {copied}\")\n",
    "    print(f\"Missing: {len(missing)}\")\n",
    "    if missing:\n",
    "        print(\"Missing examples (first 20):\")\n",
    "        for m in missing[:20]:\n",
    "            print(\"  -\", m)\n",
    "\n",
    "copy_images_with_seg(\n",
    "    # annotation_json=\"./Data/1200CNV.json\",\n",
    "    # annotation_json=\"./Data/CNV_521+Outlier (1).json\", \n",
    "    # annotation_json=\"./Data/550Noel.json\",\n",
    "    # annotation_json =\"./Data/instances_Train.json\",\n",
    "    annotation_json =\"./Data/subset_340.json\",\n",
    "    source_dir='/Users/ammaster10/Downloads/Test/oct2017/OCT2017 /train/CNV',\n",
    "    dest_dir=\"./340_Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c788e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 340 files in ./340_Test\n"
     ]
    }
   ],
   "source": [
    "def count_files_in_folder(folder_path, exts=None):\n",
    "    \"\"\"\n",
    "    Count the number of files in a folder, optionally filtering by extension.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder\n",
    "        exts (tuple, optional): File extensions to include. If None, count all files.\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of files in the folder\n",
    "    \"\"\"\n",
    "    path = Path(folder_path)\n",
    "    if not path.exists():\n",
    "        print(f\"Folder {folder_path} does not exist\")\n",
    "        return 0\n",
    "    \n",
    "    if exts:\n",
    "        # Count only files with specified extensions\n",
    "        count = sum(1 for f in path.glob(\"*\") if f.is_file() and f.suffix.lower() in exts)\n",
    "    else:\n",
    "        # Count all files\n",
    "        count = sum(1 for f in path.glob(\"*\") if f.is_file())\n",
    "    \n",
    "    print(f\"Found {count} files in {folder_path}\")\n",
    "    return count\n",
    "\n",
    "# Example usage\n",
    "# file_count = count_files_in_folder(\"/Users/ammaster10/Desktop/OCT2017/train/CNV\", exts=(\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"))\n",
    "# file_count = count_files_in_folder('/Users/ammaster10/Downloads/Test/oct2017/OCT2017 /train/CNV', exts=(\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"))\n",
    "file_count = count_files_in_folder('./340_Test', exts=(\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab04e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 1191\n",
      "Annotations: 819\n",
      "Unique image_ids (any): 819\n",
      "Unique image_ids (with seg): 819\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ann = json.loads(Path(\"./Data/instances_Train.json\").read_text(encoding=\"utf-8\"))\n",
    "\n",
    "imgs = ann.get(\"images\", [])\n",
    "anns = ann.get(\"annotations\", [])\n",
    "\n",
    "def has_seg(a):\n",
    "    s = a.get(\"segmentation\")\n",
    "    if isinstance(s, list):   # polygons\n",
    "        return len(s) > 0\n",
    "    if isinstance(s, dict):   # RLE\n",
    "        return bool(s.get(\"counts\")) and bool(s.get(\"size\"))\n",
    "    return False\n",
    "\n",
    "img_count            = len(imgs)\n",
    "ann_count            = len(anns)\n",
    "uniq_imgids_any      = len({a[\"image_id\"] for a in anns if \"image_id\" in a})\n",
    "uniq_imgids_with_seg = len({a[\"image_id\"] for a in anns if \"image_id\" in a and has_seg(a)})\n",
    "\n",
    "print(\"Images:\", img_count)\n",
    "print(\"Annotations:\", ann_count)\n",
    "print(\"Unique image_ids (any):\", uniq_imgids_any)\n",
    "print(\"Unique image_ids (with seg):\", uniq_imgids_with_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0b6b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./Data/1700Training_WithVal.json\n",
      "Total images: 2397\n",
      "Total annotations: 2575\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# <<< set your input files here >>>\n",
    "INPUTS = [\n",
    "    \"./Data/1700Training.json\",\n",
    "    \"./Validation/TEMP_VAL_45.json\"\n",
    "]\n",
    "OUT_PATH = \"./Data/1700Training_WithVal.json\"\n",
    "\n",
    "def load_json(p):\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def merge_coco(paths, out_path):\n",
    "    # Output skeleton (use first file's info/licenses if present)\n",
    "    base = load_json(paths[0])\n",
    "    out = {\n",
    "        \"info\": base.get(\"info\", {}),\n",
    "        \"licenses\": base.get(\"licenses\", []),\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [],\n",
    "    }\n",
    "\n",
    "    # Maps for ID remapping\n",
    "    cat_name_to_newid = {}\n",
    "    img_fname_to_newid = {}\n",
    "\n",
    "    # Next IDs\n",
    "    next_cat_id = 1\n",
    "    next_img_id = 1\n",
    "    next_ann_id = 1\n",
    "\n",
    "    # Build categories map across files by name\n",
    "    def get_or_add_category(cat):\n",
    "        nonlocal next_cat_id\n",
    "        name = cat[\"name\"]\n",
    "        if name not in cat_name_to_newid:\n",
    "            cat_name_to_newid[name] = next_cat_id\n",
    "            out[\"categories\"].append({\n",
    "                \"id\": next_cat_id,\n",
    "                \"name\": name,\n",
    "                \"supercategory\": cat.get(\"supercategory\", \"\")\n",
    "            })\n",
    "            next_cat_id += 1\n",
    "        return cat_name_to_newid[name]\n",
    "\n",
    "    # Process each file\n",
    "    for p in paths:\n",
    "        data = load_json(p)\n",
    "\n",
    "        # Build oldCatId -> newCatId for this file\n",
    "        cat_id_map = {}\n",
    "        for c in data.get(\"categories\", []):\n",
    "            cat_id_map[c[\"id\"]] = get_or_add_category(c)\n",
    "\n",
    "        # Images: dedupe by file_name\n",
    "        img_id_map = {}\n",
    "        for img in data.get(\"images\", []):\n",
    "            fname = img.get(\"file_name\")\n",
    "            if fname in img_fname_to_newid:\n",
    "                new_id = img_fname_to_newid[fname]\n",
    "            else:\n",
    "                new_id = next_img_id\n",
    "                img_fname_to_newid[fname] = new_id\n",
    "                new_img = dict(img)\n",
    "                new_img[\"id\"] = new_id\n",
    "                out[\"images\"].append(new_img)\n",
    "                next_img_id += 1\n",
    "            img_id_map[img[\"id\"]] = new_id\n",
    "\n",
    "        # Annotations: remap image_id and category_id, assign new ann id\n",
    "        for ann in data.get(\"annotations\", []):\n",
    "            if ann.get(\"image_id\") not in img_id_map:\n",
    "                # skip orphaned annotation\n",
    "                continue\n",
    "            if ann.get(\"category_id\") not in cat_id_map:\n",
    "                # skip unknown category\n",
    "                continue\n",
    "            new_ann = dict(ann)\n",
    "            new_ann[\"id\"] = next_ann_id\n",
    "            new_ann[\"image_id\"] = img_id_map[ann[\"image_id\"]]\n",
    "            new_ann[\"category_id\"] = cat_id_map[ann[\"category_id\"]]\n",
    "            out[\"annotations\"].append(new_ann)\n",
    "            next_ann_id += 1\n",
    "\n",
    "    # Save merged\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Totals\n",
    "    total_images = len(out[\"images\"])\n",
    "    total_annotations = len(out[\"annotations\"])\n",
    "    print(f\"Saved: {out_path}\")\n",
    "    print(f\"Total images: {total_images}\")\n",
    "    print(f\"Total annotations: {total_annotations}\")\n",
    "    return total_images, total_annotations\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_coco(INPUTS, OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ecb474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved subset:    ./Data/subset_340.json | images=340, anns=506\n",
      "Saved remainder: ./Data/remainder_1400.json | images=2057, anns=2069\n"
     ]
    }
   ],
   "source": [
    "# split_coco_subset_and_remainder.py\n",
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "def load(p): \n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f: \n",
    "        return json.load(f)\n",
    "\n",
    "def save(obj, p):\n",
    "    Path(p).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def build_coco_from_image_ids(data, keep_image_ids, remap_ids=True):\n",
    "    images_all = {im[\"id\"]: im for im in data[\"images\"]}\n",
    "    anns_all   = data[\"annotations\"]\n",
    "    cats_all   = {c[\"id\"]: c for c in data[\"categories\"]}\n",
    "\n",
    "    # images to keep\n",
    "    keep_images = [images_all[i] for i in keep_image_ids if i in images_all]\n",
    "\n",
    "    # annotations to keep\n",
    "    keep_anns = [a for a in anns_all if a[\"image_id\"] in keep_image_ids]\n",
    "\n",
    "    # categories used\n",
    "    used_cat_ids = {a[\"category_id\"] for a in keep_anns}\n",
    "    keep_cats = [cats_all[cid] for cid in used_cat_ids if cid in cats_all]\n",
    "\n",
    "    if not remap_ids:\n",
    "        return {\n",
    "            \"info\": data.get(\"info\", {}),\n",
    "            \"licenses\": data.get(\"licenses\", []),\n",
    "            \"images\": keep_images,\n",
    "            \"annotations\": keep_anns,\n",
    "            \"categories\": keep_cats,\n",
    "        }\n",
    "\n",
    "    # remap IDs to contiguous\n",
    "    oldimg2new = {oid: i+1 for i, oid in enumerate(sorted(keep_image_ids))}\n",
    "    oldcat2new = {oid: i+1 for i, oid in enumerate(sorted(used_cat_ids))}\n",
    "\n",
    "    images_out = []\n",
    "    for im in keep_images:\n",
    "        nim = dict(im); nim[\"id\"] = oldimg2new[im[\"id\"]]\n",
    "        images_out.append(nim)\n",
    "\n",
    "    anns_out = []\n",
    "    for i, a in enumerate(keep_anns, start=1):\n",
    "        na = dict(a)\n",
    "        na[\"id\"] = i\n",
    "        na[\"image_id\"] = oldimg2new[a[\"image_id\"]]\n",
    "        na[\"category_id\"] = oldcat2new[a[\"category_id\"]]\n",
    "        anns_out.append(na)\n",
    "\n",
    "    cats_out = []\n",
    "    # preserve names/supercategory while assigning new contiguous ids\n",
    "    for old_id, new_id in oldcat2new.items():\n",
    "        c = cats_all[old_id]\n",
    "        nc = {\"id\": new_id, \"name\": c[\"name\"], \"supercategory\": c.get(\"supercategory\", \"\")}\n",
    "        cats_out.append(nc)\n",
    "\n",
    "    return {\n",
    "        \"info\": data.get(\"info\", {}),\n",
    "        \"licenses\": data.get(\"licenses\", []),\n",
    "        \"images\": images_out,\n",
    "        \"annotations\": anns_out,\n",
    "        \"categories\": cats_out,\n",
    "    }\n",
    "\n",
    "def pick_annotated_images(data, n, seed=42):\n",
    "    # only images that have at least one annotation\n",
    "    img_has_ann = {im[\"id\"]: False for im in data[\"images\"]}\n",
    "    for a in data[\"annotations\"]:\n",
    "        iid = a.get(\"image_id\")\n",
    "        if iid in img_has_ann:\n",
    "            img_has_ann[iid] = True\n",
    "    pool = [im[\"id\"] for im in data[\"images\"] if img_has_ann.get(im[\"id\"], False)]\n",
    "    if n > len(pool):\n",
    "        raise ValueError(f\"Requested {n} images but only {len(pool)} annotated images available.\")\n",
    "    random.seed(seed)\n",
    "    return set(random.sample(pool, n))\n",
    "\n",
    "def split_subset_and_remainder(in_path, subset_out, remainder_out, n_subset=340, seed=42, remap_ids=True):\n",
    "    data = load(in_path)\n",
    "\n",
    "    subset_ids = pick_annotated_images(data, n_subset, seed=seed)\n",
    "    all_ids = {im[\"id\"] for im in data[\"images\"]}\n",
    "    remainder_ids = all_ids - subset_ids\n",
    "\n",
    "    subset_coco = build_coco_from_image_ids(data, subset_ids, remap_ids=remap_ids)\n",
    "    remainder_coco = build_coco_from_image_ids(data, remainder_ids, remap_ids=remap_ids)\n",
    "\n",
    "    save(subset_coco, subset_out)\n",
    "    save(remainder_coco, remainder_out)\n",
    "\n",
    "    print(f\"Saved subset:    {subset_out} | images={len(subset_coco['images'])}, anns={len(subset_coco['annotations'])}\")\n",
    "    print(f\"Saved remainder: {remainder_out} | images={len(remainder_coco['images'])}, anns={len(remainder_coco['annotations'])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    split_subset_and_remainder(\n",
    "        in_path=\"./Data/1700Training_WithVal.json\",\n",
    "        subset_out=\"./Data/subset_340.json\",\n",
    "        remainder_out=\"./Data/remainder_1400.json\",\n",
    "        n_subset=340,\n",
    "        seed=42,\n",
    "        remap_ids=True,  # set False to keep original IDs\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
