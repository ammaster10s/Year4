{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TreeBased\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  # tune to your cores\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.conf.set(\"spark.ui.showConsoleProgress\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd70819",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Index\", LongType(), True),\n",
    "    StructField(\"Arrival_Time\", LongType(), True),\n",
    "    StructField(\"Creation_Time\", LongType(), True),\n",
    "    StructField(\"x\", DoubleType(), True),\n",
    "    StructField(\"y\", DoubleType(), True),\n",
    "    StructField(\"z\", DoubleType(), True),\n",
    "    StructField(\"User\", StringType(), True),\n",
    "    StructField(\"Model\", StringType(), True),\n",
    "    StructField(\"Device\", StringType(), True),\n",
    "    StructField(\"gt\", StringType(), True),\n",
    "])\n",
    "\n",
    "# <- change these to your paths\n",
    "paths = {\n",
    "    \"accel_phone\": \"/content/drive/MyDrive/Bigdata (1)/Phones_accelerometer.csv\",\n",
    "    \"gyro_phone\":  \"/content/drive/MyDrive/Bigdata (1)/Phones_gyroscope.csv\",\n",
    "    \"accel_watch\": \"/content/drive/MyDrive/Bigdata (1)/Watch_accelerometer.csv\",\n",
    "    \"gyro_watch\":  \"/content/drive/MyDrive/Bigdata (1)/Watch_gyroscope.csv\",\n",
    "}\n",
    "\n",
    "def read_with(sensor, device_kind, path):\n",
    "    return (\n",
    "        spark.read.csv(path, header=True, schema=schema)\n",
    "             .withColumn(\"sensor\", lit(sensor))           # 'accel' | 'gyro'\n",
    "             .withColumn(\"device_kind\", lit(device_kind))  # 'phone' | 'watch'\n",
    "    )\n",
    "\n",
    "raw = (\n",
    "    read_with(\"accel\", \"phone\", paths[\"accel_phone\"])\n",
    "    .unionByName(read_with(\"gyro\",  \"phone\", paths[\"gyro_phone\"]))\n",
    "    .unionByName(read_with(\"accel\", \"watch\", paths[\"accel_watch\"]))\n",
    "    .unionByName(read_with(\"gyro\",  \"watch\", paths[\"gyro_watch\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2824f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp formatting\n",
    "ts_sec = when(col(\"Creation_Time\") > 1e14, col(\"Creation_Time\")/1e9).when(col(\"Creation_Time\") > 1e11, col(\"Creation_Time\")/1e3).otherwise(col(\"Creation_Time\").cast(\"double\"))\n",
    "\n",
    "df = (raw\n",
    "      .withColumn(\"ts_sec\", ts_sec)\n",
    "      .withColumn(\"ts\", to_timestamp(from_unixtime(col(\"ts_sec\"))))\n",
    "      .withColumn(\"m\", sqrt(col(\"x\")**2 + col(\"y\")**2 + col(\"z\")**2))\n",
    "      .filter(col(\"gt\").isNotNull() & (col(\"gt\") != \"null\"))\n",
    "     ).cache()\n",
    "\n",
    "df.select(min(\"ts\").alias(\"min_ts\"), max(\"ts\").alias(\"max_ts\")).show()\n",
    "df.groupBy(\"gt\").count().orderBy(desc(\"count\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be43dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile parquet file @ https://drive.google.com/drive/folders/1txVGk0HsBMto9J9rr0rbLoEm7DkvVgEu?usp=sharing\n",
    "# (df\n",
    "#  .repartition(\"User\",\"Device\",\"sensor\")\n",
    "#  .write.mode(\"overwrite\").partitionBy(\"User\",\"Device\",\"sensor\")\n",
    "#  .parquet(\"/content/drive/MyDrive/Bigdata\"))\n",
    "# df = spark.read.parquet(\"/content/drive/MyDrive/Bigdata-Compile\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7aa27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to 2014-2016 \n",
    "df_clean = (df\n",
    "    .filter((F.year(\"ts\") >= 2014) & (F.year(\"ts\") <= 2016))\n",
    "    .cache())\n",
    "\n",
    "print(\"Before:\", df.count(), \"rows\")\n",
    "print(\"After:\", df_clean.count(), \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "from pyspark.sql.functions import window, col, count, avg, stddev_samp, min, max, corr, lit\n",
    "from pyspark.sql.window import Window\n",
    "wdef = window(col(\"ts\"), \"2.56 seconds\", \"1.28 seconds\")\n",
    "\n",
    "# Feature lists\n",
    "feat = (df_clean.groupBy(\"User\",\"Device\",\"Model\",\"sensor\",\"window_id\")\n",
    "        .agg(\n",
    "            count(lit(1)).alias(\"n\"),\n",
    "            avg(\"x\").alias(\"x_mean\"), stddev_samp(\"x\").alias(\"x_std\"),\n",
    "            avg(\"y\").alias(\"y_mean\"), stddev_samp(\"y\").alias(\"y_std\"),\n",
    "            avg(\"z\").alias(\"z_mean\"), stddev_samp(\"z\").alias(\"z_std\"),\n",
    "            avg(\"m\").alias(\"m_mean\"), stddev_samp(\"m\").alias(\"m_std\"),\n",
    "            min(\"m\").alias(\"m_min\"), max(\"m\").alias(\"m_max\"),\n",
    "            avg(col(\"m\")*col(\"m\")).alias(\"m_energy\"),\n",
    "            corr(\"x\",\"y\").alias(\"xy_corr\"),\n",
    "            corr(\"y\",\"z\").alias(\"yz_corr\"),\n",
    "            corr(\"x\",\"z\").alias(\"xz_corr\"),\n",
    "        )\n",
    "        .filter(col(\"n\") >= 10)\n",
    ")\n",
    "\n",
    "\n",
    "lab_counts = (df_clean.groupBy(\"User\",\"Device\",\"Model\",\"window_id\",\"gt\")\n",
    "                .agg(F.count(\"*\").alias(\"cnt\")))\n",
    "\n",
    "rk = Window.partitionBy(\"User\",\"Device\",\"Model\",\"window_id\").orderBy(F.desc(\"cnt\"))\n",
    "\n",
    "labels = (lab_counts.withColumn(\"r\", F.row_number().over(rk))\n",
    "          .filter(col(\"r\")==1)\n",
    "          .select(\"User\",\"Device\",\"Model\",\"window_id\",\"gt\")\n",
    "          .withColumnRenamed(\"gt\",\"label\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join feature from accel + gyro\n",
    "feat_acc = (feat.filter(col(\"sensor\")==\"accel\")\n",
    "            .drop(\"sensor\")\n",
    "            .toDF(*[c if c in {\"User\",\"Device\",\"Model\",\"window_id\"}\n",
    "                    else c+\"_acc\" for c in feat.columns if c!=\"sensor\"]))\n",
    "\n",
    "feat_gyro = (feat.filter(col(\"sensor\")==\"gyro\")\n",
    "             .drop(\"sensor\")\n",
    "             .toDF(*[c if c in {\"User\",\"Device\",\"Model\",\"window_id\"}\n",
    "                     else c+\"_gyro\" for c in feat.columns if c!=\"sensor\"]))\n",
    "\n",
    "fused = (feat_acc.join(feat_gyro, [\"User\",\"Device\",\"Model\",\"window_id\"], \"outer\")\n",
    "               .na.fill(0.0))\n",
    "\n",
    "data = fused.select(\"User\",\"Device\",\"Model\",\"window_id\", *[c for c in fused.columns if c not in {\"User\",\"Device\",\"Model\",\"window_id\"}]).join(labels, [\"User\",\"Device\",\"Model\",\"window_id\"], \"inner\")\n",
    "\n",
    "print(\"Final joined windows:\", data.count())\n",
    "data.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns\n",
    "feature_cols = [c for c in data.columns if c.endswith(\"_acc\") or c.endswith(\"_gyro\")]\n",
    "\n",
    "data = data.select(\"window_id\", \"User\", \"Device\", \"Model\", \"label\", *feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Fold split by User\n",
    "from pyspark.sql.functions import abs as spark_abs, hash as spark_hash\n",
    "\n",
    "k = 5\n",
    "data = data.withColumn(\"fold\", (spark_abs(spark_hash(\"User\")) % k))\n",
    "train = data.filter(col(\"fold\") != 0)\n",
    "test  = data.filter(col(\"fold\") == 0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
